{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fded1e-60db-46d6-848e-89e7d1819523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zlib\n",
    "import cProfile\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from numpy.linalg import *\n",
    "from scipy import signal, stats, io\n",
    "from scipy.signal import hilbert, resample, detrend\n",
    "from scipy.stats import ranksums, ttest_ind, entropy, pearsonr, f_oneway, ttest_rel\n",
    "from scipy.io import savemat, loadmat\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "from random import *\n",
    "from itertools import combinations\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold, LeaveOneOut, train_test_split\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from pyentrp import entropy as ent\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073020fc-aaf9-4357-a7e6-df7138a08d9c",
   "metadata": {},
   "source": [
    "### Statistical Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671def65-dafc-4c7f-9019-0dbf34609a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(istring: str, dl: int, sigma: float = 0.05, method: str = \"overlapping\",\n",
    "                                     states_provided: bool = False, return_states: bool = False):\n",
    "    \"\"\"\n",
    "    Find the (forwards) Statistical Complexity of an input string for given lambda and sigma values\n",
    "    \"\"\"\n",
    "    #if states are not provided, find them, otherwise declare it\n",
    "    if(type(states_provided)==bool):\n",
    "        #first, find all states from the input string and the probabilities of presents\n",
    "        initial_states = find_states(istring,dl,method=method)\n",
    "        #next, collapse states which have similar probability distributions\n",
    "        refined_states = collapse_states(initial_states,dl,sigma)\n",
    "    else:\n",
    "        #collapse the states based purely on keynames (already done)\n",
    "        initial_states,refined_states = states_provided,states_provided\n",
    "    #convert this into a list of probabilities\n",
    "    probs = collapse_past(refined_states)\n",
    "    #create an array of logbase2 probabilities for use in the calculation\n",
    "    logprobs = np.log2(probs)\n",
    "    #sum the negative of probability * log2 probability of each state to get the statistical complexity\n",
    "    complexity = 0.0\n",
    "    for i in range(len(probs)):\n",
    "        complexity -= probs[i]*logprobs[i]\n",
    "    #if states are not desired, only return complexity\n",
    "    if(return_states==False):\n",
    "        return complexity\n",
    "    else:\n",
    "        return complexity,refined_states,initial_states\n",
    "\n",
    "def calculate_multi(istrings,dl: int, sigma: float = 0.05):\n",
    "    \"\"\"\n",
    "    Find the (forwards) Statistical Complexity values for an array of strings\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for string in istrings:\n",
    "        output.append(calculate(string,dl,sigma))\n",
    "    return output\n",
    "\n",
    "def calculate_bd(istring: str, dl: int, sigma: float = 0.05, method: str =\"overlapping\",\n",
    "                              record_states: bool = False):\n",
    "    \"\"\"\n",
    "    Find the forwards, reverse and bidirectional statistical complexity for a string\n",
    "    \"\"\"\n",
    "    #find statistical complexity of forward string and the refined states\n",
    "    f_sc,f_states,f_states_raw = calculate(istring,dl,sigma,return_states=True,method=method)\n",
    "    #find complexity of backwards string\n",
    "    b_sc,b_states,b_states_raw = calculate(istring[::-1],dl,sigma,return_states=True,method=method)\n",
    "    #collapse the states of forward and reverse complexity based purely on key names\n",
    "    bd_s = collapse_keys(f_states,b_states)\n",
    "    #find complexity of bi-directional machine\n",
    "    bd_sc,bd_states,bd_states_raw = calculate(\"\",dl,sigma,states_provided = bd_s,return_states = True)\n",
    "    if(record_states==False):\n",
    "        return f_sc,b_sc,bd_sc\n",
    "    else:\n",
    "        return f_sc,b_sc,bd_sc,len(f_states),len(b_states),len(bd_states),len(f_states_raw),len(b_states_raw),len(bd_states_raw)\n",
    "\n",
    "#Find multiple statistical complexities for forwards, reverse and bidirectional\n",
    "def calculate_bd_multi(istrings,dl: int, sigma: float = 0.05):\n",
    "    \"\"\"\n",
    "    Find the forwards, reverse and bidirectional Statistical Complexity values for an array of strings\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for string in istrings:\n",
    "        output.append(calculate_bd(string,dl,sigma))\n",
    "    return output\n",
    "\n",
    "#input string, desired lambda\n",
    "def find_states(istring: str, dl: int, method: str = \"nonoverlapping\"):\n",
    "    \"\"\"\n",
    "    Find the states present in a given input string, outputting the past states, their frequency, and their present state distributions\n",
    "    \"\"\"\n",
    "    #variables used\n",
    "    i,output_dict = 0,{}\n",
    "    ## main loop of identifying past and present states\n",
    "    # Option 1: Non Overlapping\n",
    "    if(method==\"nonoverlapping\"):\n",
    "        while(i+(dl*2)<len(istring)):\n",
    "            past,present = istring[i:i+dl],istring[i+dl:i+(2*dl)]\n",
    "            if(past not in output_dict):\n",
    "                output_dict.update({past: {present:1,\"total\":1}})\n",
    "            else:\n",
    "                if(present not in output_dict[past]):\n",
    "                    output_dict[past].update({present:1})\n",
    "                else:\n",
    "                    output_dict[past][present]+=1\n",
    "                output_dict[past][\"total\"]+=1\n",
    "            i+=dl\n",
    "    #Option 2: Overlapping\n",
    "    else:\n",
    "        while(i+dl+1<len(istring)):\n",
    "            past,present = istring[i:i+dl],istring[i+1:i+1+dl]\n",
    "            if(past not in output_dict):\n",
    "                output_dict.update({past: {present:1,\"total\":1}})\n",
    "            else:\n",
    "                if(present not in output_dict[past]):\n",
    "                    output_dict[past].update({present:1})\n",
    "                else:\n",
    "                    output_dict[past][present]+=1\n",
    "                output_dict[past][\"total\"]+=1\n",
    "            i+=1\n",
    "    # The last state discovery (main loop misses the final state that can be found)\n",
    "    try:\n",
    "        if(method==\"nonoverlapping\"):\n",
    "            past,present = istring[i:i+dl],istring[i+dl:i+(2*dl)]\n",
    "        else:\n",
    "            past,present = istring[i:i+dl],istring[i+1:i+1+dl]\n",
    "        if(past not in output_dict):\n",
    "            output_dict.update({past: {present:1,\"total\":1}})\n",
    "        else:\n",
    "            if(present not in output_dict[past]):\n",
    "                output_dict[past].update({present:1})\n",
    "            else:\n",
    "                output_dict[past][present]+=1\n",
    "            output_dict[past][\"total\"]+=1\n",
    "    except:\n",
    "        pass\n",
    "    #collapse counts into probabilities with total counts\n",
    "    for past in output_dict:\n",
    "        for present in output_dict[past]:\n",
    "            if(present!=\"total\"):\n",
    "                output_dict[past][present]/=output_dict[past][\"total\"]\n",
    "    return output_dict\n",
    "\n",
    "def collapse_states(odict: dict, dl: int, sigma: float = 0.1):\n",
    "    \"\"\"\n",
    "    Collapse a dictionary of state counts into practical states based on a permitted difference sigma\n",
    "    \"\"\"\n",
    "    # Newdict is the collapsed dictionary, temp is used to override newdict when necessary,\n",
    "    # done_checker is a dictionary used to record the keys already compared\n",
    "    newdict,temp,done_checker = deepcopy(odict),False,{}\n",
    "    while(True):\n",
    "        # If temp is not a boolean, i.e. it's become a dictionary, override newdict\n",
    "        if(type(temp)!=bool):\n",
    "            newdict = deepcopy(temp)\n",
    "            temp = False\n",
    "        for past1 in newdict:\n",
    "            if(past1 not in done_checker):\n",
    "                done_checker[past1] = {}\n",
    "            for past2 in newdict:\n",
    "                if(past1!=past2):\n",
    "                    if(past2 not in done_checker[past1]):\n",
    "                        done_checker[past1][past2] = True\n",
    "                    else:\n",
    "                        # If these states have already been checked, don't bother checking them again\n",
    "                        continue\n",
    "                    # If the difference is less than sigma, merge these states and break the past2 loop\n",
    "                    if(calculate_difference(newdict[past1],newdict[past2])<sigma):\n",
    "                        temp = merge_states(newdict,past1,past2,dl)\n",
    "                        break\n",
    "            # If the dictionary must be updated, break the past1 loop\n",
    "            if(type(temp)!=bool):\n",
    "                break\n",
    "        # If temp is still a boolean, i.e. there was no merging of states in this loop, break the main loop\n",
    "        if(type(temp)==bool):\n",
    "            break\n",
    "    return newdict\n",
    "\n",
    "def calculate_difference(past1: dict, past2: dict):\n",
    "    \"\"\"\n",
    "    Calculate the difference between two past states' present state distributions\n",
    "    \"\"\"\n",
    "    difference = -np.inf\n",
    "    # Cycle through past 1 and check for max differences\n",
    "    for present in past1:\n",
    "        # Ensure this check isn't being performed on the total state count\n",
    "        if(present!=\"total\"):\n",
    "            if(present not in past2):\n",
    "                difference = max(difference,past1[present])\n",
    "            else:\n",
    "                difference = max(difference,abs(past1[present]-past2[present]))\n",
    "    # Cycle through past 2\n",
    "    for present in past2:\n",
    "        if(present!=\"total\"):\n",
    "            if(present not in past1):\n",
    "                difference = max(difference,past2[present])\n",
    "    return difference\n",
    "\n",
    "def merge_states(odict: dict, past1: dict, past2: dict, dl: int):\n",
    "    \"\"\"\n",
    "    Merge 2 states and their present state distributions, creating a new state key in a standardised manner\n",
    "    \"\"\"\n",
    "    nprobs = {}\n",
    "    for present in odict[past1]:\n",
    "        if(present in odict[past2]):\n",
    "            nprobs.update({present:(odict[past1][present]+odict[past2][present])/2})\n",
    "        else:\n",
    "            nprobs.update({present:odict[past1][present]/2})\n",
    "    for present in odict[past2]:\n",
    "        if(present not in odict[past1]):\n",
    "            nprobs.update({present:odict[past2][present]/2})\n",
    "    nprobs.update({\"total\":odict[past1][\"total\"]+odict[past2][\"total\"]})\n",
    "    #create a sorted version of the two pasts combined\n",
    "    temp,to_sort = past1+past2,[]\n",
    "    for i in range(int(len(temp)/dl)):\n",
    "        to_sort.append(temp[(i*dl):(i*dl)+dl])\n",
    "    newkey = \"\"\n",
    "    while(len(to_sort)>0):\n",
    "        newkey += to_sort.pop(to_sort.index(min(to_sort)))\n",
    "    #add the new key and remove the old ones\n",
    "    ndict = deepcopy(odict)\n",
    "    ndict.update({newkey:nprobs})\n",
    "    del ndict[past1]\n",
    "    del ndict[past2]\n",
    "    return ndict\n",
    "\n",
    "#collapse dictionary of past states and future states into an array of probabilities of the past states\n",
    "def collapse_past(odict):\n",
    "    \"\"\"\n",
    "    Collapse a dictionary of past states with present state distribtuions into an array of probabilities of the past states\n",
    "    \"\"\"\n",
    "    probs,i,total = np.zeros(len(odict),dtype=float),0,0\n",
    "    for past in odict:\n",
    "        probs[i] += odict[past][\"total\"]\n",
    "        total += odict[past][\"total\"]\n",
    "        i+=1\n",
    "    return probs/total\n",
    "\n",
    "def collapse_keys(d1: dict, d2: dict):\n",
    "    \"\"\"\n",
    "    Merge 2 dictionaries of past states with present state distributions\n",
    "    Note: keys present in both dictionaries lose their probability distributions and only the \"total\" key remains,\n",
    "          but this is all that is needed by the point they are merged\n",
    "    \"\"\"\n",
    "    ndict = {}\n",
    "    for key in d1:\n",
    "        # as longer keys are created and sorted in a standard way, they are all standardised\n",
    "        temp = False\n",
    "        if(key in d2):\n",
    "            #if the key is in d2, add the two together and mark this has been done\n",
    "            ndict.update({key:{\"total\":d1[key][\"total\"]+d2[key][\"total\"]}})\n",
    "            temp = True\n",
    "        #if no variant of the key was found in d2, add the key as-is\n",
    "        if(temp==False):\n",
    "            ndict.update({key:d1[key]})\n",
    "    for key in d2:\n",
    "        if(key not in ndict):\n",
    "            ndict.update({key:d2[key]})\n",
    "    return ndict\n",
    "\n",
    "def probs_to_complexity(probs):\n",
    "    \"\"\"\n",
    "    Calculate the Statistical Complexity given an array of past state probabilities\n",
    "    \"\"\"\n",
    "    #create an array of logbase2 probabilities for use in the calculation\n",
    "    logprobs = np.log2(probs)\n",
    "    # Correct any 0-probabilities to become 0 rather than infinity\n",
    "    for i in range(len(logprobs)):\n",
    "        if(probs[i]==0): logprobs[i] = 0.\n",
    "    #sum the negative of probability * log2 probability of each state to get the statistical complexity\n",
    "    complexity = 0.0\n",
    "    for i in range(len(probs)):\n",
    "        complexity -= probs[i]*logprobs[i]\n",
    "    return complexity\n",
    "\n",
    "def binarise(data,mode=\"median\"):\n",
    "    \"\"\"\n",
    "    Binarise an array of continuous numbers into an array of 0's and 1's (as a string)\n",
    "    \"\"\"\n",
    "    if(type(data)==list or type(data)==tuple):\n",
    "        data=np.array(data,dtype=float)\n",
    "    if(isinstance(data,np.ndarray)==False):\n",
    "        return \"Unusable datatype {}\".format(type(data))\n",
    "    if(mode==\"median\"):\n",
    "        threshold=np.median(data)\n",
    "    elif(mode==\"mean\"):\n",
    "        threshold=np.mean(data)\n",
    "    output = np.zeros([len(data)],dtype=int)\n",
    "    for i in range(len(data)):\n",
    "        if(data[i]>=threshold):\n",
    "            output[i]+=1\n",
    "    # Convert to string\n",
    "    outstr = \"\"\n",
    "    for element in output:\n",
    "        outstr += str(element)\n",
    "    return outstr\n",
    "\n",
    "def multi_binarise(matrix,mode=\"median\"):\n",
    "    \"\"\"\n",
    "    Binarise a 2D matrix (used for calculating multiple statistical complexities)    \n",
    "    \"\"\"\n",
    "    print(\"Binarising data...\")\n",
    "    # Convert to numpy matrix\n",
    "    if(type(matrix)==list or type(matrix)==tuple):\n",
    "        matrix=np.array(matrix,dtype=object)\n",
    "    output = []\n",
    "    percent_check = 0.\n",
    "    for i in range(len(matrix)):\n",
    "        output.append(binarise(matrix[i],mode))\n",
    "        # Output progress\n",
    "        if((i+1)/float(len(matrix))>=percent_check/100.):\n",
    "            print(\"{}% of data binarised\".format(\\\n",
    "                round(((i+1)/len(matrix))*100,1)))\n",
    "            percent_check+=10.\n",
    "    print(\"Data Binarised\")\n",
    "    return np.array(output, dtype = object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0accc9c-566b-4da1-a899-7825c0bcf6f3",
   "metadata": {},
   "source": [
    "### Lempel-Ziv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ad9b2-ed1c-489a-9340-cc4c51e957b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Python code to compute LZc complexity measure as described in \"Complexity of multi-dimensional spontaneous EEG decreases during propofol induced general anaesthesia\"\n",
    "\n",
    "Author: m.schartner@sussex.ac.uk\n",
    "Date: 09.12.14\n",
    "\n",
    "To compute Lempel-Ziv complexity for continuous multidimensional time series X, where rows are time series (minimum 2), and columns are observations, type the following in ipython: \n",
    " \n",
    "execfile('CompMeasures.py')\n",
    "LZc(X)\n",
    "'''\n",
    "\n",
    "def Pre(X):\n",
    " '''\n",
    " Detrend and normalize input data, X a multidimensional time series\n",
    " '''\n",
    " ro,co=shape(X)\n",
    " Z=zeros((ro,co))\n",
    " for i in range(ro):\n",
    "  Z[i,:]=signal.detrend(X[i,:]-mean(X[i,:]), axis=0)\n",
    " return Z\n",
    "\n",
    "\n",
    "##########\n",
    "'''\n",
    "LZc - Lempel-Ziv Complexity, column-by-column concatenation\n",
    "'''\n",
    "##########\n",
    "\n",
    "def cpr(string):\n",
    "    \"\"\"\n",
    "    Lempel-Ziv-Welch compression of binary input string, e.g., string='0010101'.\n",
    "    It outputs the size of the dictionary of binary words.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    w = ''\n",
    "    for c in string:\n",
    "        wc = w + c\n",
    "        if wc in d:\n",
    "            w = wc\n",
    "        else:\n",
    "            d[wc] = wc\n",
    "            w = c\n",
    "    return len(d)\n",
    "\n",
    "def LZc_binary(string):\n",
    "    \"\"\"\n",
    "    Compute LZ complexity for a binary string and normalize it using a shuffled result.\n",
    "    \"\"\"\n",
    "    # Shuffling the string\n",
    "    shuffled_string = list(string)\n",
    "    np.random.shuffle(shuffled_string)\n",
    "    shuffled_string = ''.join(shuffled_string)\n",
    "    \n",
    "    # Calculate complexities\n",
    "    original_complexity = cpr(string)\n",
    "    shuffled_complexity = cpr(shuffled_string)\n",
    "    \n",
    "    # Normalize the complexity by the shuffled result if shuffled_complexity is not zero\n",
    "    if shuffled_complexity != 0:\n",
    "        normalized_complexity = original_complexity / float(shuffled_complexity)\n",
    "    else:\n",
    "        normalized_complexity = original_complexity\n",
    "\n",
    "    return normalized_complexity\n",
    "\n",
    "\n",
    "def str_col(X):\n",
    " '''\n",
    " Input: Continuous multidimensional time series\n",
    " Output: One string being the binarized input matrix concatenated comlumn-by-column\n",
    " '''\n",
    " ro,co=shape(X)\n",
    " TH=zeros(ro)\n",
    " M=zeros((ro,co))\n",
    " for i in range(ro):\n",
    "  M[i,:]=abs(hilbert(X[i,:]))\n",
    "  TH[i]=mean(M[i,:])\n",
    "\n",
    " s=''\n",
    " for j in xrange(co):\n",
    "  for i in xrange(ro):\n",
    "   if M[i,j]>TH[i]:\n",
    "    s+='1'\n",
    "   else:\n",
    "    s+='0'\n",
    "\n",
    " return s\n",
    "\n",
    "def LZc(X):\n",
    " '''\n",
    " Compute LZc and use shuffled result as normalization\n",
    " '''\n",
    " X=Pre(X)\n",
    " SC=str_col(X)\n",
    " M=list(SC)\n",
    " shuffle(M)\n",
    " w=''\n",
    " for i in range(len(M)):\n",
    "  w+=M[i]\n",
    " return cpr(SC)/float(cpr(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd9a74-f541-4cc7-ace9-742bf4e7b3c7",
   "metadata": {},
   "source": [
    "### Step 1: Load and Downsample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c001c-185b-4346-ac1e-fba9bed641d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_downsample(filepath, target_freq=250):\n",
    "    # Load the data from the .mat file\n",
    "    data = sio.loadmat(filepath)['dat']\n",
    "    original_freq = 1000  # Original frequency in Hz\n",
    "    downsample_factor = original_freq // target_freq  # Calculate the downsample factor\n",
    "    # Downsample the data to the target frequency\n",
    "    downsampled_data = resample(data, data.shape[1] // downsample_factor, axis=1)\n",
    "    return downsampled_data\n",
    "\n",
    "base_path = 'C:/Users/odans/Documents/SUSSEX LAB WORKS/Dissertation Analysis'\n",
    "participants = ['ba', 'fe', 'fr', 'gi', 'me', 'pa', 'pe', 'te', 'to', 'za']\n",
    "states = ['E', 'L', 'R', 'W']\n",
    "\n",
    "def load_all_data(base_path, participants, states, target_freq=250):\n",
    "    all_data = {}  # Initialize a dictionary to store all data\n",
    "    for participant in participants:\n",
    "        all_data[participant] = {}  # Initialize a dictionary for each participant\n",
    "        participant_path = os.path.join(base_path, participant)  # Construct the participant's path\n",
    "        for state in states:\n",
    "            filename = f\"{state}1000.mat\"  # Construct the filename for the state\n",
    "            filepath = os.path.join(participant_path, filename)  # Construct the full filepath\n",
    "            # Load and downsample the data for the current state\n",
    "            downsampled_data = load_and_downsample(filepath, target_freq)\n",
    "            all_data[participant][state] = downsampled_data  # Store the downsampled data\n",
    "    return all_data\n",
    "\n",
    "# Load all data for all participants and states\n",
    "all_data = load_all_data(base_path, participants, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4803b480-0e69-4808-9269-72bc7cd0afa3",
   "metadata": {},
   "source": [
    "### Step 2: Segment the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf19bd48-9db4-48c7-9d0c-4c0e3ff462c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_data(data, segment_length, overlap=0):\n",
    "    segments = []\n",
    "    step = segment_length - overlap\n",
    "    for start in range(0, data.shape[1] - segment_length + 1, step):\n",
    "        segments.append(data[:, start:start + segment_length])\n",
    "    return np.array(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81e7ecf-0415-48a5-b0c3-d2bba7aa4507",
   "metadata": {},
   "source": [
    "### Step 3: Calculate Complexity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5a573-a212-4e4a-8e11-76c2951d3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_segment(segment, mode=\"median\"):\n",
    "    binarized_segment = []\n",
    "    for channel in segment:\n",
    "        # Binarize each channel in the segment\n",
    "        binarized_segment.append(binarise(channel, mode))\n",
    "    # Join the binarized channels into a single string\n",
    "    return ''.join(binarized_segment)\n",
    "\n",
    "def calculate_complexity_measures(segments, complexity_function, is_statistical, **kwargs):\n",
    "    complexity_measures = []\n",
    "    for segment in segments:\n",
    "        # Binarize the segment\n",
    "        binarized_segment = binarize_segment(segment)\n",
    "        if is_statistical:\n",
    "            # Calculate statistical complexity\n",
    "            complexity = complexity_function(binarized_segment, **kwargs)[0]\n",
    "        else:\n",
    "            # Calculate non-statistical complexity\n",
    "            complexity = complexity_function(binarized_segment)\n",
    "        # Append the complexity measure to the list\n",
    "        complexity_measures.append(complexity)\n",
    "    # Return the mean complexity measure\n",
    "    return np.mean(complexity_measures, axis=0)\n",
    "\n",
    "segment_length = 250 * 2  # Segment length in samples (e.g., 2 seconds segments)\n",
    "overlap = 0  # No overlap between segments\n",
    "\n",
    "statistical_complexity_results = {}\n",
    "lz_complexity_results = {}\n",
    "\n",
    "for participant in participants:\n",
    "    statistical_complexity_results[participant] = {}\n",
    "    lz_complexity_results[participant] = {}\n",
    "    for state in states:\n",
    "        # Get the data for the current participant and state\n",
    "        data = all_data[participant][state]\n",
    "        # Segment the data\n",
    "        segments = segment_data(data, segment_length, overlap)\n",
    "        # Calculate statistical complexity measures\n",
    "        statistical_complexity = calculate_complexity_measures(segments, calculate_bd, is_statistical=True, dl=7)\n",
    "        # Calculate Lempel-Ziv complexity measures\n",
    "        lz_complexity = calculate_complexity_measures(segments, LZc_binary, is_statistical=False)\n",
    "        # Store the results in the dictionaries\n",
    "        statistical_complexity_results[participant][state] = statistical_complexity\n",
    "        lz_complexity_results[participant][state] = lz_complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee26eea-1394-4fa5-8a99-134368dff9fb",
   "metadata": {},
   "source": [
    "### Step 4: Compute Grand Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506f6f9c-1633-4014-a599-d2867f64126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grand_mean(results):\n",
    "    grand_means = {}\n",
    "    for participant in results:\n",
    "        grand_means[participant] = {}\n",
    "        for state in results[participant]:\n",
    "            grand_means[participant][state] = np.mean(results[participant][state])\n",
    "    return grand_means\n",
    "\n",
    "grand_mean_statistical_complexity = compute_grand_mean(statistical_complexity_results)\n",
    "grand_mean_lz_complexity = compute_grand_mean(lz_complexity_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f25b4a-f4cc-4329-a63d-61cf902d3160",
   "metadata": {},
   "source": [
    "### Step 5: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d053e4-fda3-4437-be81-d467ce967e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(grand_means, title):\n",
    "    df = pd.DataFrame(grand_means)\n",
    "    print(f\"\\n{title}\\n\", df)\n",
    "\n",
    "display_results(grand_mean_statistical_complexity, \"Grand Mean Statistical Complexity\")\n",
    "display_results(grand_mean_lz_complexity, \"Grand Mean Lempel-Ziv Complexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573287c3-368c-4c03-8e1e-db2576d3b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(grand_means, title):\n",
    "    # Extract participants and states from the grand_means dictionary\n",
    "    participants = list(grand_means.keys())\n",
    "    states = list(grand_means[participants[0]].keys())\n",
    "    \n",
    "    # Calculate the mean and standard error for each state\n",
    "    means = np.array([[grand_means[participant][state] for state in states] for participant in participants])\n",
    "    mean_values = means.mean(axis=0)\n",
    "    std_errors = means.std(axis=0) / np.sqrt(means.shape[0])\n",
    "\n",
    "    # Create a bar plot with error bars\n",
    "    plt.figure(figsize=(4, 5))\n",
    "    plt.bar(states, mean_values, yerr=std_errors, capsize=5, color='skyblue', alpha=0.7)\n",
    "    plt.xlabel('States')\n",
    "    plt.ylabel('Complexity')\n",
    "    plt.title(title)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    file_path = r'C:\\Users\\odans\\Documents\\SUSSEX LAB WORKS\\DISSERTATION\\Images\\New folder\\GM2.png'\n",
    "    plt.savefig(file_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the results for Lempel-Ziv complexity and statistical complexity\n",
    "plot_results(grand_mean_lz_complexity, \"Grand Mean Lempel-Ziv Complexity with Standard Error\")\n",
    "plot_results(grand_mean_statistical_complexity, \"Grand Mean Statistical Complexity with Standard Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73414d-317b-4709-960a-1683395f902a",
   "metadata": {},
   "source": [
    "### Perform Paired T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b2d1b-82a0-4d51-b47f-1da2352125bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_t_tests(grand_means):\n",
    "    # Extract the list of states from the grand_means dictionary\n",
    "    states = list(grand_means[participants[0]].keys())\n",
    "    # Initialize a matrix to store p-values\n",
    "    p_values = np.zeros((len(states), len(states)))\n",
    "\n",
    "    for i in range(len(states)):\n",
    "        for j in range(i + 1, len(states)):\n",
    "            state1 = states[i]\n",
    "            state2 = states[j]\n",
    "            # Extract data for the two states across all participants\n",
    "            data1 = [grand_means[participant][state1] for participant in participants]\n",
    "            data2 = [grand_means[participant][state2] for participant in participants]\n",
    "            # Perform paired t-test\n",
    "            _, p_value = ttest_rel(data1, data2)\n",
    "            # Store the p-value in the matrix\n",
    "            p_values[i, j] = p_value\n",
    "            p_values[j, i] = p_value\n",
    "    \n",
    "    return p_values, states\n",
    "\n",
    "def plot_heatmap(p_values, states, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Create a heatmap of the p-values\n",
    "    sns.heatmap(p_values, annot=True, xticklabels=states, yticklabels=states, cmap=\"viridis\", cbar=True, fmt=\".4f\")\n",
    "    plt.title(title)\n",
    "    # Save the heatmap to a file\n",
    "    file_path = r'C:\\Users\\odans\\Documents\\SUSSEX LAB WORKS\\DISSERTATION\\Images\\New folder\\GM_H1.png'\n",
    "    plt.savefig(file_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Perform t-tests and plot heatmap for Lempel-Ziv Complexity\n",
    "p_values_lz, states = perform_t_tests(grand_mean_lz_complexity)\n",
    "plot_heatmap(p_values_lz, states, \"T-Test P-Values for Lempel-Ziv Complexity\")\n",
    "\n",
    "# Perform t-tests and plot heatmap for Statistical Complexity\n",
    "p_values_statistical, states = perform_t_tests(grand_mean_statistical_complexity)\n",
    "plot_heatmap(p_values_statistical, states, \"T-Test P-Values for Statistical Complexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff8782-2c15-430b-b143-dbb00eee6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize t-tests\n",
    "def plot_grand_mean_with_error_bars(grand_mean_complexity, title):\n",
    "    # Extract the list of states from the grand_mean_complexity dictionary\n",
    "    states = list(grand_mean_complexity[participants[0]].keys())\n",
    "    # Calculate the mean and standard error for each state\n",
    "    means = np.array([[grand_mean_complexity[participant][state] for state in states] for participant in participants])\n",
    "    mean_values = means.mean(axis=0)\n",
    "    std_errors = means.std(axis=0) / np.sqrt(means.shape[0])\n",
    "\n",
    "    # Create an error bar plot\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    plt.errorbar(states, mean_values, yerr=std_errors, fmt='o', color='blue', ecolor='red', elinewidth=2, capsize=5)\n",
    "    plt.xlabel('Sleep Stages')\n",
    "    plt.ylabel('Complexity')\n",
    "    plt.title(title)\n",
    "    # Save the plot to a file\n",
    "    file_path = r'C:\\Users\\odans\\Documents\\SUSSEX LAB WORKS\\DISSERTATION\\Images\\New folder\\t_t1.png'\n",
    "    plt.savefig(file_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the grand mean with error bars for Lempel-Ziv complexity and statistical complexity\n",
    "plot_grand_mean_with_error_bars(grand_mean_lz_complexity, \"Lempel-Ziv Complexity Across Sleep Stages\")\n",
    "plot_grand_mean_with_error_bars(grand_mean_statistical_complexity, \"Statistical Complexity Across Sleep Stages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39d1fa-19c4-4535-b1ee-7b1354414374",
   "metadata": {},
   "source": [
    "### Comparison With Kolmogorov Complexity and Approximate Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b65fca0-e742-46f1-a805-d747d88ea6f1",
   "metadata": {},
   "source": [
    "####  Parameter Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aabb7e-4d02-42a1-a2be-6e73cb38c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing complexity measures\n",
    "def binarise(data, mode=\"median\"):\n",
    "    binary_data = (data >= np.median(data, axis=1, keepdims=True)).astype(int)\n",
    "    return ''.join(binary_data.flatten().astype(str))\n",
    "\n",
    "def LZc_binary(binary_string):\n",
    "    # Assuming binary_string is a long string of 0s and 1s\n",
    "    return len(zlib.compress(binary_string.encode('utf-8')))\n",
    "\n",
    "# complexity measures\n",
    "def calculate_kolmogorov_complexity(binary_string):\n",
    "    compressed_data = zlib.compress(binary_string.encode('utf-8'))\n",
    "    return len(compressed_data)\n",
    "\n",
    "def approximate_entropy(U, m=2, r=0.2):\n",
    "    def _maxdist(x_i, x_j):\n",
    "        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "\n",
    "    N = len(U)\n",
    "    x = [U[i: i + m] for i in range(N - m + 1)]\n",
    "    C = [(sum([_maxdist(x_i, x_j) <= r for x_j in x]) - 1) / (N - m) for x_i in x]\n",
    "    return np.log(np.sum(C) / (N - m + 1))\n",
    "\n",
    "# Updated apply_complexity_measures_with_params function\n",
    "def apply_complexity_measures_with_params(data, dl=5, sigma=0.05, segment_length=250):\n",
    "    binarized_data = binarise(data)\n",
    "    lz_complexity = LZc_binary(binarized_data)\n",
    "    stat_diversity = calculate(binarized_data, dl=dl, sigma=sigma)\n",
    "    kolmogorov_complexity = calculate_kolmogorov_complexity(binarized_data)\n",
    "    apen = approximate_entropy(data.flatten())  # Assuming data is 1D\n",
    "    return lz_complexity, stat_diversity, kolmogorov_complexity, apen\n",
    "\n",
    "# Updated analyze_with_varied_parameters function\n",
    "def analyze_with_varied_parameters(base_path, participants, states, dls, sigmas, segment_lengths):\n",
    "    results = {}\n",
    "    for participant in participants:\n",
    "        results[participant] = {}\n",
    "        for state in states:\n",
    "            filepath = f\"{base_path}/{participant}/{state}1000.mat\"\n",
    "            downsampled_data = load_and_downsample(filepath)\n",
    "            results[participant][state] = {}\n",
    "            for dl in dls:\n",
    "                for sigma in sigmas:\n",
    "                    for segment_length in segment_lengths:\n",
    "                        key = (dl, sigma, segment_length)\n",
    "                        truncated_data = downsampled_data[:, :segment_length]\n",
    "                        lz_complexity, stat_diversity, kolmogorov_complexity, apen = apply_complexity_measures_with_params(\n",
    "                            truncated_data, dl=dl, sigma=sigma, segment_length=segment_length)\n",
    "                        results[participant][state][key] = (lz_complexity, stat_diversity, kolmogorov_complexity, apen)\n",
    "    return results\n",
    "\n",
    "# Parameters to vary\n",
    "dls = [3, 4, 5, 7]\n",
    "sigmas = [0.05, 0.1]\n",
    "segment_lengths = [250, 500, 1000, 2000]  # Example segment lengths in number of samples\n",
    "base_path = 'C:/Users/odans/Documents/SUSSEX LAB WORKS/Dissertation Analysis'\n",
    "participants = ['ba', 'fe', 'fr', 'gi', 'me', 'pa', 'pe', 'te', 'to', 'za']\n",
    "states = ['E', 'L', 'R', 'W']\n",
    "\n",
    "# Example usage\n",
    "results_varied_params = analyze_with_varied_parameters(base_path, participants, states, dls, sigmas, segment_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6597b7-86ca-416d-aef7-2349561a9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to collect data\n",
    "data = []\n",
    "\n",
    "# Iterate over the results to extract relevant data\n",
    "for participant, states in results_varied_params.items():\n",
    "    for state, params in states.items():\n",
    "        for (dl, sigma, segment_length), values in params.items():\n",
    "            if len(values) == 2:  # Ensure both Kolmogorov and ApproxEntropy are present\n",
    "                kolmogorov, approximate_entropy = values\n",
    "                data.append({\n",
    "                    'Participant': participant,\n",
    "                    'State': state,\n",
    "                    'dl': dl,\n",
    "                    'sigma': sigma,\n",
    "                    'Segment_Length': segment_length,\n",
    "                    'Kolmogorov_Complexity': kolmogorov,\n",
    "                    'Approximate_Entropy': approximate_entropy\n",
    "                })\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07494205-6344-493a-a2d6-63b3ae7b0163",
   "metadata": {},
   "source": [
    "### Perform T-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d7726-62c7-45d9-a822-11091d143b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform t-tests for Kolmogorov Complexity and Approximate Entropy\n",
    "def perform_t_tests(df, state1, state2):\n",
    "    results = {}\n",
    "    \n",
    "    # Filter the DataFrame for the two states\n",
    "    df_state1 = df[df['State'] == state1]\n",
    "    df_state2 = df[df['State'] == state2]\n",
    "    \n",
    "    # Perform t-test for Kolmogorov Complexity\n",
    "    t_stat_kolmogorov, p_value_kolmogorov = ttest_ind(df_state1['Kolmogorov_Complexity'], df_state2['Kolmogorov_Complexity'], equal_var=False)\n",
    "    \n",
    "    # Perform t-test for Approximate Entropy\n",
    "    t_stat_apen, p_value_apen = ttest_ind(df_state1['Approximate_Entropy'], df_state2['Approximate_Entropy'], equal_var=False)\n",
    "    \n",
    "    # Store the results\n",
    "    results['Kolmogorov_Complexity'] = {\n",
    "        't_statistic': t_stat_kolmogorov,\n",
    "        'p_value': p_value_kolmogorov\n",
    "    }\n",
    "    results['Approximate_Entropy'] = {\n",
    "        't_statistic': t_stat_apen,\n",
    "        'p_value': p_value_apen\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage: Compare states 'E' and 'L'\n",
    "t_test_results = perform_t_tests(df, 'E', 'L')\n",
    "\n",
    "# Display the t-test results\n",
    "print(\"T-test results for Kolmogorov Complexity and Approximate Entropy between states 'E' and 'L':\")\n",
    "print(t_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c9536-fd4f-499e-b3e1-f017c321b10d",
   "metadata": {},
   "source": [
    "### Visualise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024593a-9095-41ed-8d2b-77e0a05258aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Comparison Dict\n",
    "states_to_compare = ['E', 'L', 'R', 'W']\n",
    "comparison_results = {}\n",
    "\n",
    "for i in range(len(states_to_compare)):\n",
    "    for j in range(i + 1, len(states_to_compare)):\n",
    "        state1 = states_to_compare[i]\n",
    "        state2 = states_to_compare[j]\n",
    "        comparison_results[f'{state1} vs {state2}'] = perform_t_tests(df, state1, state2)\n",
    "\n",
    "\n",
    "\n",
    "# Function to plot t-test results\n",
    "def plot_t_test_results(comparison_results):\n",
    "    comparisons = list(comparison_results.keys())\n",
    "    kolmogorov_t_stats = [comparison_results[comp]['Kolmogorov_Complexity']['t_statistic'] for comp in comparisons]\n",
    "    kolmogorov_p_values = [comparison_results[comp]['Kolmogorov_Complexity']['p_value'] for comp in comparisons]\n",
    "    apen_t_stats = [comparison_results[comp]['Approximate_Entropy']['t_statistic'] for comp in comparisons]\n",
    "    apen_p_values = [comparison_results[comp]['Approximate_Entropy']['p_value'] for comp in comparisons]\n",
    "    \n",
    "    x = np.arange(len(comparisons))  # the label locations\n",
    "\n",
    "    # Plot T-statistics\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    ax[0].bar(x - 0.2, kolmogorov_t_stats, 0.4, label='Kolmogorov Complexity', color='blue', alpha=0.7)\n",
    "    ax[0].bar(x + 0.2, apen_t_stats, 0.4, label='Approximate Entropy', color='orange', alpha=0.7)\n",
    "    ax[0].set_ylabel('T-statistic')\n",
    "    ax[0].set_title('T-statistics for Kolmogorov Complexity and Approximate Entropy')\n",
    "    ax[0].set_xticks(x)\n",
    "    ax[0].set_xticklabels(comparisons, rotation=45, ha='right')\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot P-values\n",
    "    ax[1].bar(x - 0.2, kolmogorov_p_values, 0.4, label='Kolmogorov Complexity', color='blue', alpha=0.7)\n",
    "    ax[1].bar(x + 0.2, apen_p_values, 0.4, label='Approximate Entropy', color='orange', alpha=0.7)\n",
    "    ax[1].set_ylabel('P-value')\n",
    "    ax[1].set_title('P-values for Kolmogorov Complexity and Approximate Entropy')\n",
    "    ax[1].axhline(0.05, color='red', linestyle='--', label='Significance Level (0.05)')\n",
    "    ax[1].set_xticks(x)\n",
    "    ax[1].set_xticklabels(comparisons, rotation=45, ha='right')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Now plot the results\n",
    "plot_t_test_results(comparison_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f0421-3091-4b48-b598-f32b98609737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means and standard errors for each state for Kolmogorov Complexity\n",
    "kolmogorov_means = df.groupby('State')['Kolmogorov_Complexity'].mean()\n",
    "kolmogorov_std_errors = df.groupby('State')['Kolmogorov_Complexity'].sem()  # Standard Error of the Mean\n",
    "\n",
    "# Calculate means and standard errors for Approximate Entropy\n",
    "apen_means = df.groupby('State')['Approximate_Entropy'].mean()\n",
    "apen_std_errors = df.groupby('State')['Approximate_Entropy'].sem()\n",
    "\n",
    "# Sort states to ensure correct order in the plot\n",
    "states = ['E', 'L', 'R', 'W']\n",
    "kolmogorov_means = kolmogorov_means.reindex(states)\n",
    "kolmogorov_std_errors = kolmogorov_std_errors.reindex(states)\n",
    "apen_means = apen_means.reindex(states)\n",
    "apen_std_errors = apen_std_errors.reindex(states)\n",
    "\n",
    "# Plotting side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot for Kolmogorov Complexity\n",
    "axes[0].errorbar(states, kolmogorov_means, yerr=kolmogorov_std_errors, fmt='o', color='blue', ecolor='red', capsize=5)\n",
    "axes[0].set_title('Kolmogorov Complexity Across Sleep Stages')\n",
    "axes[0].set_xlabel('Sleep Stages')\n",
    "axes[0].set_ylabel('Complexity')\n",
    "axes[0].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot for Approximate Entropy\n",
    "axes[1].errorbar(states, apen_means, yerr=apen_std_errors, fmt='o', color='blue', ecolor='red', capsize=5)\n",
    "axes[1].set_title('Approximate Entropy Across Sleep Stages')\n",
    "axes[1].set_xlabel('Sleep Stages')\n",
    "axes[1].set_ylabel('Complexity')\n",
    "axes[1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
